# 260102
## ğŸ”¥ChallengeğŸ’§
- ğŸ’§ì¼ì° ì¶œê·¼: 11:00
- ğŸ”¥1ì¼1ë…¼ë¬¸: Accelerating Scientific Discovery with Autonomous Goal-evolving Agents ë§ˆë¬´ë¦¬
- 1ì¼1ë…¼ë¬¸: TAID: Temporally Adaptive Interpolated Distillation for Efficient Knowledge Transfer in Language Models (ICLR 2025 Spotlight)
- 1ì¼1êµ¬í˜„: cross-lingual RAG (https://arxiv.org/abs/2505.10089)
  - https://github.com/amazon-science/XRAG?utm_source=chatgpt.com 

## âœ…TO-DOğŸ 
- âœ…ELMo) ë””ë ‰í† ë¦¬ ì •ë¦¬ í›„ ì»¤ë°‹
- âœ…ELMo) ì „ì²˜ë¦¬ í™•ì¸ -> ë‹¤ì‹œ ì§„í–‰ ì¤‘ 
- âœ…ELMo) íŠ¸ëŸ¬ë¸” ìŠˆíŒ… ì‘ì„± -> ë°ì´í„° ì „ì²˜ë¦¬ ì‹œ UNK í† í° ê°œìˆ˜ 2000ê°œ ëŒ€ 
- Transformers) wikidocs 3, 4
- âœ…Transformers) ref code ì°¾ê¸° https://ysg2997.tistory.com/11 https://nlp.seas.harvard.edu/2018/04/03/attention.html 
- ë””ë™) â€œThink Twiceâ€: Perspective-Taking Improves LLMsâ€™ Theory-of-Mind Capabilities â€“ ACL 2024 ì½ê¸° 

## ğŸ“ŒToday I Learned
### ëª¨ë¸ ë³„ íŠ¹ì„± ì°¾ì•„ë³´ê¸°


## ğŸ’¡ íšŒê³  / ì¸ì‚¬ì´íŠ¸

## ğŸ’¥ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…
### biLM Collapse ë²„ê·¸
1. í•™ìŠµê³¼ í‰ê°€ ì‹œ ë‹¤ë¥¸ Projection ì‚¬ìš©
- í•™ìŠµí•  ë• forward_projection, backward_projectionìœ¼ë¡œ ê°ê° í•™ìŠµ
- í‰ê°€í•  ë•Œ get_representations ë©”ì„œë“œë¥¼ í†µí•´ projection ê°€ì ¸ì˜¤ëŠ”ë°, ì´ë•Œ lstm_projection ê°€ì ¸ì™”ìŒ

2. tied_weight ë¬´ë¶„ë³„ ì‚¬ìš©
- tied_weightë€ ì…ë ¥ ì„ë² ë”© ê°€ì¤‘ì¹˜ì™€ ì¶œë ¥ ê°€ì¤‘ì¹˜ë¥¼ ë™ì¼í•˜ê²Œ ì“°ëŠ” ê¸°ë²•
- ì°¨ì›ì„ ë§ì¶”ë ¤ê³  ì„ì‹œ í•´ê²°ì±…ìœ¼ë¡œ ì ìš©í–ˆì„ ê²ƒ

3. ì˜ëª»ëœ dropout ìœ„ì¹˜
- projection ì´í›„

4. â­í•™ìŠµ ì‹œ ëŒ€ë¶€ë¶„ í† í°ì´ UNK (85%)
```bash
forward_target unique labels: (
    tensor([1, 6, 9, 10, 6793, 20688, ...], device='cuda:0'),  # â† ê³ ìœ  label ê°’ë“¤
    tensor([2635, 4, 4, 8, 1, 1, 1, ...], device='cuda:0')     # â† ê° labelì˜ ê°œìˆ˜
)
```
- Label 1 (UNK)ê°€ 2635ê°œ(ì „ì²´ í† í° 3072ê°œ)
- `word2idx = limited_word2idx`ê°€ ë¬¸ì œë¨
    - ëŒ€ì²´ ì™œ ì¸ë±ì‹± ê³¼ì •ì—ì„œ 100kë¡œ ì œí•œí•œ ê±¸ ë®ì–´ì”Œìš´ ê±¸ê¹Œìš”
    - â­â­â­ì¸ë±ì‹± ê³¼ì •ì—ì„  ì›ë³¸ ë°ì´í„° ì¨ì•¼ í•¨
    - corpus â†’ 3.5M(ì›ë³¸) vocabìœ¼ë¡œ ì¸ë±ì‹± â†’ ì›ë³¸ ì¸ë±ìŠ¤(0~3.5M) â†’ ì¬ë§¤í•‘: ë¹ˆë„ ê¸°ì¤€(ë‚®ì€ ë¹ˆë„ â†’ UNK)
    - ì´ì „ì—ëŠ” ì½”í¼ìŠ¤ ë‹¨ì–´ë“¤ì„ 100K vocabìœ¼ë¡œ ì¸ë±ì‹± í–ˆìŒ â‡’ ëŒ€ë¶€ë¶„ì˜ ë‹¨ì–´ê°€ UNKê°€ ë¨


## ğŸ©ë‚´ì¼ í•  ì¼ 
- Transformers) wikidocs 3, 4
- ë””ë™) â€œThink Twiceâ€: Perspective-Taking Improves LLMsâ€™ Theory-of-Mind Capabilities â€“ ACL 2024 ì½ê¸° 
- ELMo) epoch 1 eval (16ì‹œ ì˜ˆìƒ)
- Transformers) ë¸”ë¡œê·¸ ì½ê¸° https://ysg2997.tistory.com/11 https://nlp.seas.harvard.edu/2018/04/03/attention.html 
- CS224N) 1ê°• ìˆ˜ê°• 
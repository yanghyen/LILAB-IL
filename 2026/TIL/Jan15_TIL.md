# 260115
## ğŸ”¥ChallengeğŸ’§
- ğŸ’§ì¼ì° ì¶œê·¼: 12:30
- 1ì¼1ë…¼ë¬¸: LittleBit: Ultra Low-Bit Quantization via Latent Factorization, Angles Don't Lie: Unlocking Training-Efficient RL Through the Model's Own Signals
- 1ì¼1êµ¬í˜„: cross-lingual RAG (https://arxiv.org/abs/2505.10089)
  - https://github.com/amazon-science/XRAG?utm_source=chatgpt.com 

## âœ…TO-DOğŸ 
- ì§„í–‰ì‚¬ì•ˆ ì—…ë¡œë“œ
- ELMo) í•™ìŠµí•œ ê²ƒë“¤ fixed char
- ELMo) í•™ìŠµëœ ê²ƒë“¤ test
- ELMo) í•™ìŠµëœ ê²ƒë“¤ downstream task 
- Transformers) ê°œë… ê³µë¶€
  - Transformers) ë¸”ë¡œê·¸ ì½ê¸° https://ysg2997.tistory.com/11 https://nlp.seas.harvard.edu/2018/04/03/attention.html 
- ë””ë™) â€œThink Twiceâ€: Perspective-Taking Improves LLMsâ€™ Theory-of-Mind Capabilities â€“ ACL 2024 ì½ê¸° 

## ğŸ“ŒToday I Learned
### ëª¨ë¸ ë³„ íŠ¹ì„± ì°¾ì•„ë³´ê¸°


## ğŸ’¡ íšŒê³  / ì¸ì‚¬ì´íŠ¸

## ğŸ’¥ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…


## ğŸ©ë‚´ì¼ í•  ì¼ 
- ì§„í–‰ì‚¬ì•ˆ ì—…ë¡œë“œ
- ELMo) í•™ìŠµí•œ ê²ƒë“¤ fixed char
- ELMo) í•™ìŠµëœ ê²ƒë“¤ test
- ELMo) í•™ìŠµëœ ê²ƒë“¤ downstream task 
- Transformers) ê°œë… ê³µë¶€
  - Transformers) ë¸”ë¡œê·¸ ì½ê¸° https://ysg2997.tistory.com/11 https://nlp.seas.harvard.edu/2018/04/03/attention.html 
- ë””ë™) â€œThink Twiceâ€: Perspective-Taking Improves LLMsâ€™ Theory-of-Mind Capabilities â€“ ACL 2024 ì½ê¸° 
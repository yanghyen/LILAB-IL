# 251203
## âœ…TO-DOğŸ 
- âœ…ELMo ê°œë… í•™ìŠµ
    - âœ…ë…¼ë¬¸ ì½ê¸°(5.3~)
- ELMo ë ˆí¼ëŸ°ìŠ¤ ì½”ë“œ ì‹¤í–‰í•´ë³´ê¸°
- 10ì›” MIL ì‘ì„±
- 11ì›” MIL ì‘ì„± 
- ì‹¬ë¦¬í•™ ì—°êµ¬ ë…¼ë¬¸1 ì½ê¸° 

## ğŸ“ŒToday I Learned
### ELMo ì‚¬ì „í•™ìŠµ
1. Character embedding (char lookup)
2. Character-level CNN (char-CNN)
3. Highway networks (2-layer)
4. Projection (to LSTM input, 512d)
5. 2-layer bidirectional LSTM (biLM)
6. Softmax LM heads (forward & backward)
```
raw text -> tokenized words -> each word -> chars -> char embeddings
  -> char-CNN convs -> concat -> highway -> projection(512)  # layer0 (512)
  -> biLSTM layer1 -> proj -> h^1 (1024)                     # layer1
  -> biLSTM layer2 -> proj -> h^2 (1024)                     # layer2
  -> LM heads -> forward/backward softmax (compute LM loss)
```

## ğŸ’¡ íšŒê³  / ì¸ì‚¬ì´íŠ¸

## ğŸ’¥ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

## ğŸ©ë‚´ì¼ í•  ì¼ 
# 251024
## ✅TO-DO
- data load 파트 -> 스트리밍 저장 적용
- SGNS 트러블슈팅 작성 
- Word2Vec paper 작성을 위해 논문 파악 ~ 
- 평가지표 정리 

## 📌Today I Learned

## 💡 회고 / 인사이트
### AI 환경에 익숙해지기
- 익숙해져야 할 AI 환경이 많다. 
- 객관적 평가 지표, 데이터셋 구축 및 전처리, 디렉토리 구조

### 객관적 평가 지표(Intern Project)
#### accuracy/F1

#### BLEU for MT

####  perplexity for LMs

#### intrinsic similarity/analogy for embeddings

### 데이터셋 구축 및 전처리
- 데이터셋 구축도 쉽지 않다. 처음엔 위키피디아에서 제공하는 뉴스 압축파일을 받아서 직접 전처리하려고 했다. 근데 파일 구조가 깔끔하지 않고 태그가 정말 다양하게 많아서 전처리가 잘 안됐다. 
- **허깅페이스**를 이용하자. 이 생각을 왜 이제 했나 몰랑.

### 디렉토리 구조
- 이건 그냥 하다보니 익숙해졌다. 다른 논문 재구현하면서 자주 보고, 내가 구현하려 하다보니 익숙해졌다. 여전히 경로 설정은 헷갈리지만 스크립트 실행같은 게 익숙해지고 있다. 

### 스트리밍을 위한 개념들
#### Generator & yeild

#### IterableDataset



## 💥 트러블슈팅
### 문제상황
- 데이터셋 크기가 3배 증가해서 학습이 50일 걸린다 ㄴㅇㄱ....
- 현재 데이터를 load하는 과정은 파일을 통째로 읽어서 굉장히 비효율적이다.```load_corpus_and_tokenize```
### 해결방안
- 코퍼스를 다 불러와서 메모리에 올리지 말고, 한 줄씩 불러와서 처리하자.
- ```process_corpus_and_stream```제너레이터 도입
- 원본 파일에서 문서를 하나씩 읽고 처리하여 토큰들을 순차적으로 ```yield```하는 제너레이터 함수 도입
- 저장 방식은 .pkl -> .txt로 변경. 그래야 스트리밍이 가능 
- ```Dataset``` -> ```IterableDataset```

### 문제상황
- ```train.py```에서 토큰화된 코퍼스가 필요하다. <- ```pretrain.py```에서 임시파일이ㅏㄹ 삭제하고 바이너리 데이터만 남김
- 데이터를 스트리밍하니까 ```train.py```를 할 때는 전체 코퍼스를 다시 불러와서 전처리를 해야 ```vocab.pkl```을 생성할 수 있다.
- 데이터셋이 규모가 크기 때문에 두번 불러오는 건 비효율적임
### 해결방안
- ```pretrain.py```에서 ```vocab.pkl```을 생성하기로 했다.
## 🍩내일 할 일 
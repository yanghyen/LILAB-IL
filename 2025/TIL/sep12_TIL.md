# 250912
## ✅TO-DO
- ✅위키독스 09-04
- ✅위키독스 09-05
- ✅위키독스 09-06
- 밑딥시 ch3
- CBOW 구조에서 원-핫 벡터와 가중치 W와 행렬곱을 진행한 벡터의 평균을 구하는 걸까? -> 답 찾아보기
- 논문 Abstract, Conclusion 읽어보기

## 📌Today I Learned
### GloVe
- 단어 임베딩 방법론
- LSA와 Word2Vec의 단점 보완하는 목적
- Word2Vec만큼 뛰어난 성능
- Word2Vec은 임베딩 벡터가 윈도우 크기 내에서만 주변 단어 고려해서 코퍼스 전체적인 통계 정보 반영 못함  
-> LSA의 카운트 기반 방법과 Word2Vec의 예측 기반 방법론 모두 사용 
- 핵심: 임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것 

### FastText
- 단어를 글자 단위 n-gram의 구성으로 취급하여 내부 단어(subword) 토큰을 벡터로 만들어서 학습
- 오타에 의해서 혹은 Vocabulary에 없는 단어를 예측하려고 할 때 Word2Vec은 에러를 내지만, FastText는 해당 단어의 n-gram과 겹치는 n-gram을 가진 단어가 있다면 벡터값을 계산해서 결과 냄 

## 💡 회고 / 인사이트
### CBOW 구조에서 원-핫 벡터와 가중치 W와 행렬곱을 진행한 벡터의 평균을 구하는 걸까?

## 💥 트러블슈팅

## 🍩내일 할 일
- 주말동안 WIL, 슬랙 진행사안 업로드